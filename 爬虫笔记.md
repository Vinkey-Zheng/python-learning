## 爬取页面

### 几个重要模块

前情提要

```python
import requests # 获取页面
import pprint # 打印
import urllib
from urllib import parse # urllib模块中的解析模块
from pyquery import PyQuery as pq # pq.text()只保留文本
```

列表表达式

```python
a_List = [x * x for x in a_range] # [表达式 变量 in 范围]
```

生成器

```python
yield迭代器，可以先初步理解跟return差不多的东西，但是不一样
html = get_html()
res = parase_html(html)
print(res)  # 生成器对象
# 结果 <generator object parase_html at 0x0000021130365270>
print(next(res))
# 开始打印
```

再来看一个例子

```python
def foo():
    print("starting...")
    while True:
        res = yield 4
        print("res:",res)
g = foo() # 不会打印，只是得出来一个生成器对象
print(next(g)) # 开始打印，返回是4，然后退出
print("*"*20)
print(next(g)) # 因为已经退出了，所以不会返回4，而是None
# 如果在执行next()，又会return一个4，所以下一个打印出来又是4

结果：
starting...
4
********************
res: None
4
```

来看看整块获取网页的代码，用到requests

```python
def get_html():
    url
    response = requests.get(url=url) # 一般用get获取
    res = response.content  # 二进制文件,打印出来有'b',表示的是二进制bytes
    #print(type(res))
    res = response.text  # 字符串
    #print(type(res)) # str
    res = response.json() # 字典
    #print(res)
    return res
```

上面提到了requests是get网页，但其实他还有其他的用法

```python
# 防止别人知道我是爬虫，headers里面放user-agent
urllib.request.Request(url=url, data(bytes)=data, headers=headers,method="POST")
# 尤其需要user-agent
urllib.request.urllib.urlopen('http://httpbin.org/post')
# 状态码
print(response.status)# response = urllib.request.Request
print(response.getheaders())

```

解析数据，用到pyquery，yield

```python
def parase_html(html):
    cards = html["data"]["cards"] # 这些是在微博network元素中看到的一些网页数据
    for card in cards: # 循环遍历
        xinna = {} #先建立一个空字典
        mblog = card["mblog"] # card里面又有mlog标签
        # pq().text()只保留文本,去除html标签
        xinna["内容"] = pq(mblog["text"]).text()
        xinna["评论"] = mblog["comments_count"]
        # 字符串格式化的方法
        # print(f"content:{con},comment:{comment}") 
        # 类似的还有r(不转义字符),b(二进制)
        print(xinna)
        yield xinna
```

重点来看一下parse模块

```python
lis = doc('h2.content-title').items()
for li in lis:
    print(li.text())//将字典对象的text文本打印出来，也就是“生活娱乐”。。。
/***************我是一条分界线*************/    
data = bytes(urllib.parse.urlencode({"hello": "world"}), encoding = 'utf-8')
```

<img src="C:\Users\86135\AppData\Roaming\Typora\typora-user-images\image-20220128222305662.png" alt="image-20220128222305662" style="zoom:50%;" />

urlib.parse.urlencode() 将dict类型参数转化为query_string格式（key=value&key=value），并且将中文转码，最终会转换为bytes(字节流)

## 解析页面

```python
# 1.BeautifulSoup : 标签, html当中的标签,比如说head, div, a等
# 2.NavigableString  bs.a.string 标签里面的内容    字典: bs.a.attrs
# 3.BeautifulSoup 整个文档 print(bs)
# 4. Comment 输出的内容不报含注释的内容
```

### 正则表达式补充

1.有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性:

```python
data_soup = BeautifulSoup('<div data-foo="value">foo!</div>')
data_soup.find_all(data-foo="value")
# SyntaxError: keyword can't be an expression
```

但是可以通过 `find_all()` 方法的 `attrs` 参数定义一个字典参数来搜索包含特殊属性的tag:

```python
data_soup.find_all(attrs={"data-foo": "value"})
# [<div data-foo="value">foo!</div>]    表达式可以是字符串、布尔值、正则表达式
```

2. ```python
   class属性要用class_="",class=True ，表示查找所有带有class_的标签
   ```

   

find_all( [name](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#id32) , [attrs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#css) , [recursive](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#recursive) , [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#text) , [**kwargs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#keyword) )

`find_all()` 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件.这里有几个例子:

```
soup.find_all("title")
# [<title>The Dormouse's story</title>]

soup.find_all("p", "title")
# [<p class="title"><b>The Dormouse's story</b></p>]
```

```python
bs4.find_all(re.compile("a"))
bs4.find_all(class_=True)
```

### 正则表达式常用操作符

![image-20220211000210933](C:\Users\86135\AppData\Roaming\Typora\typora-user-images\image-20220211000210933.png)

![image-20220211000956869](C:\Users\86135\AppData\Roaming\Typora\typora-user-images\image-20220211000956869.png)
